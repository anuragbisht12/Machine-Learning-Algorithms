{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljB8vr1ir0An",
        "colab_type": "text"
      },
      "source": [
        "Support Vector Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1Ur_AwcsCO9",
        "colab_type": "text"
      },
      "source": [
        "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
        "\n",
        "**  The advantages of support vector machines are:**\n",
        "\n",
        "  Effective in high dimensional spaces.\n",
        "\n",
        "  Still effective in cases where number of dimensions is greater than the number of samples.\n",
        "\n",
        "  Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
        "\n",
        "  Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
        "\n",
        "\n",
        "**  The disadvantages of support vector machines include:**\n",
        "\n",
        "If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
        "\n",
        "SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7zFSSC0s7R4",
        "colab_type": "text"
      },
      "source": [
        "**1. Classification**\n",
        "SVC, NuSVC and LinearSVC are classes capable of performing binary and multi-class classification on a dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPRbhsa1sbU_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "f5201789-dd18-4537-f893-e83d2aeae437"
      },
      "source": [
        "from sklearn import svm\n",
        "X = [[0, 0], [1, 1]]\n",
        "y = [0, 1]\n",
        "clf = svm.SVC()\n",
        "clf.fit(X, y)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFIF216QvAXV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "205dbec8-6b19-481a-8907-729a5c89bad7"
      },
      "source": [
        "clf.predict([[2., 2.]])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1KGsP9jvKSy",
        "colab_type": "text"
      },
      "source": [
        "Some properties of these support vectors can be found in attributes support_vectors_, support_ and n_support_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMYS-r7fvLlx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "5150a6f3-ca7b-44aa-e171-8297eeb3f433"
      },
      "source": [
        "# get support vectors\n",
        "print(clf.support_vectors_)\n",
        "# get indices of support vectors\n",
        "print(clf.support_)\n",
        "# get number of support vectors for each class\n",
        "print(clf.n_support_)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0.]\n",
            " [1. 1.]]\n",
            "[0 1]\n",
            "[1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhTH7IYXwL6I",
        "colab_type": "text"
      },
      "source": [
        "2. Multiclass classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlaU013Hwc9y",
        "colab_type": "text"
      },
      "source": [
        " In total, n_classes * (n_classes - 1) / 2 classifiers are constructed and each one trains data from two classes. To provide a consistent interface with other classifiers, the decision_function_shape option allows to monotonically transform the results of the “one-versus-one” classifiers to a “one-vs-rest” decision function of shape (n_samples, n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe41pwE_weW3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1407e2bc-42f0-49a4-ecd2-a8292f5b2ac8"
      },
      "source": [
        "X = [[0], [1], [2], [3]]\n",
        "Y = [0, 1, 2, 3]\n",
        "clf = svm.SVC(decision_function_shape='ovo')\n",
        "clf.fit(X, Y)\n",
        "\n",
        "dec = clf.decision_function([[1]])\n",
        "print(dec.shape[1]) # 4 classes: 4*3/2 = 6\n",
        "\n",
        "clf.decision_function_shape = \"ovr\"\n",
        "dec = clf.decision_function([[1]])\n",
        "print(dec.shape[1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Es5itKhr4m9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c0e0e6b-7bc6-4a6b-998a-43cb9f0582e2"
      },
      "source": [
        "lin_clf = svm.LinearSVC()\n",
        "lin_clf.fit(X, Y)\n",
        "\n",
        "dec = lin_clf.decision_function([[1]])\n",
        "print(dec.shape[1])\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfQxO-d4zi2Z",
        "colab_type": "text"
      },
      "source": [
        "Scores and probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdmY9G-sz2Su",
        "colab_type": "text"
      },
      "source": [
        "The cross-validation involved in Platt scaling is an expensive operation for large datasets. In addition, the probability estimates may be inconsistent with the scores:\n",
        "\n",
        "the “argmax” of the scores may not be the argmax of the probabilities\n",
        "\n",
        "in binary classification, a sample may be labeled by predict as belonging to the positive class even if the output of predict_proba is less than 0.5; and similarly, it could be labeled as negative even if the output of predict_proba is more than 0.5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjtQoAog0si-",
        "colab_type": "text"
      },
      "source": [
        "Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dalWET2q03Sc",
        "colab_type": "text"
      },
      "source": [
        "The method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression.\n",
        "\n",
        "The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function ignores samples whose prediction is close to their target.\n",
        "\n",
        "There are three different implementations of Support Vector Regression: SVR, NuSVR and LinearSVR. LinearSVR provides a faster implementation than SVR but only considers the linear kernel, while NuSVR implements a slightly different formulation than SVR and LinearSVR. See Implementation details for further details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8aUIDnU1eeN",
        "colab_type": "text"
      },
      "source": [
        "As with classification classes, the fit method will take as argument vectors X, y, only that in this case y is expected to have floating point values instead of integer values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBSAXyLT06Jy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e609544-78e5-4397-be0d-bbae052b8d72"
      },
      "source": [
        "from sklearn import svm\n",
        "X = [[0, 0], [2, 2]]\n",
        "y = [0.5, 2.5]\n",
        "regr = svm.SVR()\n",
        "regr.fit(X, y)\n",
        "\n",
        "print(regr.predict([[1, 1]]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrxen-XT1O88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRmRcUj62X0M",
        "colab_type": "text"
      },
      "source": [
        "**Density estimation, novelty detection**\n",
        "The class OneClassSVM implements a One-Class SVM which is used in outlier detection.\n",
        "\n",
        "outlier detection\n",
        "The training data contains outliers which are defined as observations that are far from the others. Outlier detection estimators thus try to fit the regions where the training data is the most concentrated, ignoring the deviant observations.\n",
        "\n",
        "novelty detection\n",
        "The training data is not polluted by outliers and we are interested in detecting whether a new observation is an outlier. In this context an outlier is also called a novelty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULPBINx_33N6",
        "colab_type": "text"
      },
      "source": [
        "Outlier detection and novelty detection are both used for anomaly detection, where one is interested in detecting abnormal or unusual observations. Outlier detection is then also known as unsupervised anomaly detection and novelty detection as semi-supervised anomaly detection. In the context of outlier detection, the outliers/anomalies cannot form a dense cluster as available estimators assume that the outliers/anomalies are located in low density regions. On the contrary, in the context of novelty detection, novelties/anomalies can form a dense cluster as long as they are in a low density region of the training data, considered as normal in this context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEwpANzV47MU",
        "colab_type": "text"
      },
      "source": [
        "**Complexity**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIfusmuo42mx",
        "colab_type": "text"
      },
      "source": [
        "Support Vector Machines are powerful tools, but their compute and storage requirements increase rapidly with the number of training vectors. The core of an SVM is a quadratic programming problem (QP), separating support vectors from the rest of the training data. The QP solver used by the libsvm-based implementation scales between \n",
        " and \n",
        " depending on how efficiently the libsvm cache is used in practice (dataset dependent). If the data is very sparse  should be replaced by the average number of non-zero features in a sample vector.\n",
        "\n",
        "For the linear case, the algorithm used in LinearSVC by the liblinear implementation is much more efficient than its libsvm-based SVC counterpart and can scale almost linearly to millions of samples and/or features."
      ]
    }
  ]
}